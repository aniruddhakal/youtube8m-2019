{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "\n",
    "import heapq\n",
    "import gzip\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output filename that will be created\n",
    "OUTPUTFILE = \"3x_geometric_dbof_2vlad_blend_maskedembeddlocalization_and_2018normed_200k_labels.csv\"\n",
    "# Filenames or 5 frame predictions. 'preds.npy' and 'ids.npy' extensions are automatically added.\n",
    "FILENAMES = [\"dbof_finetune_wmask_cp752\", \"VLAD5frames_53525_542\", \"VLAD5frames_50842_686\"]\n",
    "# Location of 'segment_label_ids.csv' file\n",
    "LABELS = \"./segment_label_ids.csv\"\n",
    "\n",
    "# Location of localization predictions\n",
    "LOCALIZATION_FOLDER = \"./localization_preds\"\n",
    "#  Video model predictions'preds.npy' and 'ids.npy' extensions are automatically added.\n",
    "VIDEOMODEL = \"./2018model_whole_video\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parallel preparation of localization predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel preparation of localization predictions\n",
    "mapping_def_inx = {i: idx for i, idx in enumerate(pd.read_csv(LABELS)[\"Index\"])}\n",
    "mapping_inx_sorted = {idx:i  for i, idx in enumerate(sorted(pd.read_csv(LABELS)[\"Index\"]))}\n",
    "\n",
    "os.makedirs(\"./processed\", exist_ok=True)\n",
    "\n",
    "ref_table = []\n",
    "for i, (xpreds, xid) in tqdm(enumerate(zip(my_preds, my_ids))):\n",
    "        ref_id, pred_loc = xid.decode().split(\":\")\n",
    "        ref_table.append([int(pred_loc), ref_id])\n",
    "        \n",
    "loc_preds = os.path.join(LOCALIZATION_FOLDER, \"predfile_{}_ids.npy\")\n",
    "loc_ids = os.path.join(LOCALIZATION_FOLDER, \"predfile_{}_preds.npy\")\n",
    "def par_proc(j):\n",
    "    my_preds = np.zeros(2062258, dtype=np.float32)\n",
    "    lids = np.load(loc_ids.format(j), allow_pickle=True)\n",
    "    lpred = np.log(np.clip(np.load(loc_preds.format(j), \n",
    "                                   allow_pickle=True), 10**-7, 1))\n",
    "    idx_mapping = {xid: i for i, xid in enumerate(lids)}\n",
    "    jj = mapping_inx_sorted[mapping_def_inx[j]]\n",
    "    # def_idx -> sorted_idx\n",
    "    \n",
    "    for i, (pred_loc, ref_id) in enumerate(ref_table):\n",
    "        leq_idx = idx_mapping[ref_id]\n",
    "    \n",
    "        # Factor for localization\n",
    "        factor = np.mean(lpred[leq_idx][int(pred_loc):int(pred_loc)+5]) # TODO: should we sum or multiply?!\n",
    "        my_preds[i] += factor\n",
    "    print(\"Done {}\".format(j))\n",
    "    np.save(\"./processed/arr{}\".format(j), my_preds)\n",
    "\n",
    "from multiprocessing import Pool\n",
    "my_pool = Pool(8)\n",
    "my_pool.map(par_proc, range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights = [2/3., 2/3., 2/3.]\n",
    "top_k = 20\n",
    "segment_max_pred = 200000\n",
    "\n",
    "\n",
    "print(\"loading file\", filenames[0])\n",
    "my_preds = weights[0] * np.log(np.clip(np.load(\"{}preds.npy\".format(filenames[0])), 10**-7, 1))\n",
    "my_ids = np.load(\"{}ids.npy\".format(filenames[0]))\n",
    "\n",
    "id_order = my_ids.argsort()\n",
    "my_ids = my_ids[id_order]\n",
    "my_preds = my_preds[id_order]\n",
    "\n",
    "assert my_ids.shape == (2062258,)\n",
    "assert my_preds.shape == (2062258, 1000)\n",
    "\n",
    "for weight, new_file in zip(weights[1:], filenames[1:]):\n",
    "    print(\"loading file\", new_file)\n",
    "    new_preds = weight * np.log(np.clip(np.load(\"{}preds.npy\".format(new_file)), 10**-7, 1))\n",
    "    new_ids = np.load(\"{}ids.npy\".format(new_file))\n",
    "    \n",
    "    assert new_ids.shape == (2062258,)\n",
    "    assert new_preds.shape == (2062258, 1000)\n",
    "\n",
    "    id_order = new_ids.argsort()\n",
    "    new_ids = new_ids[id_order]\n",
    "    new_preds = new_preds[id_order]\n",
    "    \n",
    "    my_preds += new_preds \n",
    "                  \n",
    "\n",
    "idx_mapping = {xid: i for i, xid in enumerate(lids)}\n",
    "\n",
    "vl_2018preds = np.log(np.load(VIDEOMODEL + \"preds.npy\", allow_pickle=True))\n",
    "vl_2018ids = np.load(VIDEOMODEL + \"ids.npy\", allow_pickle=True)\n",
    "video_mapping = {xid: i for i, xid in enumerate(vl_2018ids)}\n",
    "\n",
    "for i, (xpreds, xid) in tqdm(enumerate(zip(my_preds, my_ids))):   \n",
    "    # Factor for video level\n",
    "    factor = vl_2018preds[video_mapping[ref_id]]\n",
    "    my_preds[i] += factor\n",
    "\n",
    "for i in trange(1000):\n",
    "    new_vals = np.load(\"./processed/arr{}.npy\".format(i))\n",
    "    my_preds[:, i] += new_vals\n",
    "    \n",
    "\n",
    "# Get dictionary order.\n",
    "idx_mapping = {i: idx for i, idx in enumerate(sorted(pd.read_csv(LABELS)[\"Index\"]))}\n",
    "\n",
    "if not OUTPUTFILE.endswith(\".gz\"): out_file += \".gz\"\n",
    "final_out_file = gzip.open(OUTPUTFILE, \"wb\")\n",
    "final_out_file.write(\"Class,Segments\\n\".encode())\n",
    "\n",
    "for i, cls in tqdm(idx_mapping.items()):\n",
    "\n",
    "    pred_group = my_preds[:, i]\n",
    "    label_top_hits = my_ids[pred_group.argsort()[-segment_max_pred:][::-1]]\n",
    "    cls_heap = [label for label in label_top_hits]\n",
    "        \n",
    "    wstring = \"%d,%s\\n\" %(cls, \" \".join([x.decode() for x in cls_heap]))\n",
    "    final_out_file.write(wstring.encode())\n",
    "    \n",
    "final_out_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
